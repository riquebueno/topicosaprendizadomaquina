{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho1TopicosAprendMaq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/riquebueno/topicosaprendizadomaquina/blob/master/Trabalho1TopicosAprendMaq.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "2K2frxTAxCiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PXcxoAqMkKbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac\n",
        "\n",
        "https://medium.com/deep-math-machine-learning-ai/chapter-9-2-nlp-code-for-word2vec-neural-network-tensorflow-544db99f5334"
      ]
    },
    {
      "metadata": {
        "id": "Bec5IvJZcMJM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Trabalho de Tópicos Avançados em Aprendizado de Máquina -Henrique Bueno Rodrigues"
      ]
    },
    {
      "metadata": {
        "id": "A1UU5TDdvW6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "8c207849-2e08-4aff-955d-31a30dba513b"
      },
      "cell_type": "code",
      "source": [
        "#Instalação de bibliotecas\n",
        "!pip install -U gensim\n",
        "!pip install -U nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 56kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 268kB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/b4/7c6af5f0574397c74e3c40dba733733f5ed086ab3dd49bf3008b6e9b3dc6/boto3-1.9.10-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting botocore<1.13.0,>=1.12.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/c5/816a626efe96a3c90b1da3b7335ae6bdaa781b0de843ea93b642fc867e63/botocore-1.12.10-py2.py3-none-any.whl (4.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.7MB 220kB/s \n",
            "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 228kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.10->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.10->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 64kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.10 botocore-1.12.10 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zmVcqC4adwVB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk as nltk\n",
        "from sklearn import datasets, linear_model\n",
        "\n",
        "# import modules & set up logging\n",
        "import gensim, logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AnARdMdXcTZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "22fc3ddb-e7cd-4934-ac90-a98dd083412d"
      },
      "cell_type": "code",
      "source": [
        "#Leitura do dataset\n",
        "#df = pd.read_csv('ted_main.csv')\n",
        "#df.ratings[0]\n",
        "df = pd.read_csv('transcripts.csv')\n",
        "#umTexto = df.transcript[0]\n",
        "#olhar depois stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "#acho que também precisarei fazer stemming: pegar do nltk.stem!!!!!!!!!!!!!!!!!!!!\n",
        "#http://thalesbertaglia.com/enelvo/sobre/ para normalização\n",
        "textos = df['transcript']\n",
        "\n",
        "#Preciso dar mais atenção à etapa de pré-processamento\n",
        "\n",
        "caracteresARemover = \"?<>\\{\\}\\(\\)!@#$.,;:\\\"-_—\"\n",
        "\n",
        "\n",
        "\n",
        "textosProcessados = []\n",
        "#Remoção de caracteres\n",
        "for texto in textos:\n",
        "  for i in range(0,len(caracteresARemover)):\n",
        "    texto = texto.lower().replace(caracteresARemover[i],\" \")\n",
        "  textosProcessados.append(texto)\n",
        "\n",
        "\n",
        "  \n",
        "#Split dos textos e eliminação de palavras vazias\n",
        "palavrasDeTodosOsTextos = []\n",
        "for texto in textosProcessados:\n",
        "  palavrasDeUmTexto = texto.split(\" \")\n",
        "  #removendo os strings vazios\n",
        "  palavrasDeUmTexto = list(filter(None, palavrasDeUmTexto))\n",
        "  palavrasDeTodosOsTextos.append(palavrasDeUmTexto)\n",
        "\n",
        "# Count word frequencies\n",
        "from collections import defaultdict\n",
        "frequency = defaultdict(int)\n",
        "for text in palavrasDeTodosOsTextos:\n",
        "    for token in text:\n",
        "        frequency[token] += 1  \n",
        "\n",
        "# Only keep words that appear more than once\n",
        "processed_corpus = [[token for token in text if frequency[token] > 1] for text in palavrasDeTodosOsTextos]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7a4832a2b8e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transcripts.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#umTexto = df.transcript[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#olhar depois stopwords = nltk.corpus.stopwords.words('portuguese')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#acho que também precisarei fazer stemming: pegar do nltk.stem!!!!!!!!!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#http://thalesbertaglia.com/enelvo/sobre/ para normalização\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at line 1764"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OcbHioltNqrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "94033ef0-3e71-4359-cb39-b43ab1d961ae"
      },
      "cell_type": "code",
      "source": [
        "#Preciso pegar as montar uma matriz onde as linhas\n",
        "#sao os textos e as colunas as palavras dos textos.\n",
        "#pegarei as 30 primeiras palavras de cada texto.\n",
        "\n",
        "\n",
        "#from numpy import array\n",
        "#palavrasDeTodosOsTextos=array(palavrasDeTodosOsTextos)\n",
        "#print(palavrasDeTodosOsTextos[0,2])\n",
        "#palavrasDeTodosOsTextosCortada = palavrasDeTodosOsTextos[:,[1,2]]\n",
        "#print(palavrasDeTodosOsTextos[0])\n",
        "#print(palavrasDeTodosOsTextosCortada[0])\n",
        "\n",
        "palavrasDeTodosOsTextos=[['google','is','a','great','company'],['google','is','the','best','place'],['thats','a','joke','man','Are','you','serious']]\n",
        "\n",
        "numeroDeLinhas=len(palavrasDeTodosOsTextos)\n",
        "print(numeroDeLinhas)\n",
        "print(palavrasDeTodosOsTextos)\n",
        "saida=[]\n",
        "for linha in palavrasDeTodosOsTextos:\n",
        "  novaLinha=[]\n",
        "  for i in range(4):\n",
        "    novaLinha.append(linha[i])\n",
        "  saida.append(novaLinha)\n",
        "print(saida)\n",
        "\n",
        "#RESOLVER O PROLBEMA DE DIFERENCAO DOS ARQUIVOS DE ENTRADA (2550 OU 2467?)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "[['google', 'is', 'a', 'great', 'company'], ['google', 'is', 'the', 'best', 'place'], ['thats', 'a', 'joke', 'man', 'Are', 'you', 'serious']]\n",
            "[['google', 'is', 'a', 'great'], ['google', 'is', 'the', 'best'], ['thats', 'a', 'joke', 'man']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v5hhttJX_sd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9c32bcec-350f-499f-c79d-fb7ad3b1bcd0"
      },
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "modelGlove = api.load(\"glove-wiki-gigaword-100\")\n",
        "#modelWord2Vec = api.load(\"word2vec-google-news-300\")\n",
        "#modelFastText = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "#como verificar se uma palavra existe em um Word2VecKeyedVectors?\n",
        "#vector = modelGlove[\"cant\"]  # numpy vector of a word\n",
        "#vector.shape\n",
        "#vector"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-09-25 18:30:49,978 : INFO : Creating /root/gensim-data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2018-09-25 18:31:29,753 : INFO : glove-wiki-gigaword-100 downloaded\n",
            "2018-09-25 18:31:29,755 : INFO : loading projection weights from /root/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n",
            "2018-09-25 18:32:26,986 : INFO : loaded (400000, 100) matrix from /root/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "US_v_AwS-pEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "bec11303-153c-4c4d-d608-7c3a124d6b40"
      },
      "cell_type": "code",
      "source": [
        "#agora vou converter a tabela de textos SAIDA para uma\n",
        "#tabela de vetores\n",
        "\n",
        "matrizDeVetores=[]\n",
        "for linha in saida:\n",
        "  linhaDeVetores=[]\n",
        "  for palavra in linha:\n",
        "    vetorDaPalavra = modelGlove[palavra]\n",
        "    #Sugestão para fazer a média do embedding\n",
        "    #http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
        "    mediaDoVetorDaPalavra=np.mean(vetorDaPalavra)\n",
        "    #linhaDeVetores.append(vetorDaPalavra)\n",
        "    linhaDeVetores.append(mediaDoVetorDaPalavra)\n",
        "  matrizDeVetores.append(linhaDeVetores)\n",
        "print(len(matrizDeVetores))\n",
        "print(len(matrizDeVetores[0]))\n",
        "print(matrizDeVetores)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "4\n",
            "[[0.037531357, 0.010669153, 0.026050145, -0.05754394], [0.037531357, 0.010669153, -0.027958173, -0.0054065417], [-0.0028232199, 0.026050145, 0.0012205958, 0.039224193]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_iSThaNDBM_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6cefb81-4131-4f78-b143-1e9b23678221"
      },
      "cell_type": "code",
      "source": [
        "#Aplicando Regressão Logística\n",
        "\n",
        "Y=['A','A','B']\n",
        "X=matrizDeVetores\n",
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression(C=1.0)\n",
        "logreg.fit(X, Y)\n",
        "print(\"Training set score: %f\" % logreg.score(X, Y))\n",
        "\n",
        "#TREINO COM REDE NEURAL\n",
        "#from sklearn.neural_network import MLPClassifier\n",
        "#mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
        "#                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "#                    learning_rate_init=.1)\n",
        "#mlp.fit(X, Y)\n",
        "#print(\"Training set score: %f\" % mlp.score(X, Y))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4fcAcHvABNLW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qCpckQOnkp14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#palavrasDeTodosOsTextos\n",
        "#frequency\n",
        "#processed_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOb3Km28IYvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from gensim import corpora\n",
        "\n",
        "#dictionary = corpora.Dictionary(processed_corpus)\n",
        "#print(dictionary)\n",
        "#45427 unique tokens: [\"'30s\", '15', '16', '1930s', '19th']...) from 2467 documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGWrGQ4-JduU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(dictionary.token2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SinIZjBCCqPI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#preciso implementar o seguinte: pegar as 30 primeiras palavras de cada texto e \n",
        "#pegar o vetor delas (farei isso para cada tipo de embedding).\n",
        "\n",
        "\n",
        "\n",
        "model = gensim.models.Word2Vec(processed_corpus, min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3fxUEXlfr07",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(model)\n",
        "#print(model.wv.vocab)\n",
        "model.most_similar(\"music\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXS2tsRQAXTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvFEN7Jxj5Je",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2381979-785f-4618-ea6f-100c19cc8c26"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-zP38lr-Rva6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "9838f37a-02e0-4ffd-a9a5-b387b6fa29fb"
      },
      "cell_type": "code",
      "source": [
        "modelGlove.train([\"henrique\", \"heloisa\"])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-73cc116e839a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelGlove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"henrique\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"heloisa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'train'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7ZZlt01tlE3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "13964b51-7553-4269-9540-629af048ded4"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.28225  ,  0.17079  , -0.44491  , -0.64913  , -1.1914   ,\n",
              "        0.17594  , -0.63942  ,  0.096859 ,  0.36172  ,  0.16236  ,\n",
              "        0.41335  , -0.0013565, -0.021236 , -0.099696 ,  0.48747  ,\n",
              "       -0.25507  , -0.24418  ,  0.5572   ,  0.048192 ,  0.017053 ,\n",
              "        0.16046  ,  0.33716  ,  0.23784  , -0.76324  , -0.45461  ,\n",
              "       -0.10025  ,  0.28172  , -0.04291  ,  0.8897   , -0.88826  ,\n",
              "       -0.17835  ,  0.36524  ,  0.38285  ,  0.27374  ,  0.38882  ,\n",
              "        0.58908  , -0.086302 ,  0.21059  ,  1.3125   ,  0.32943  ,\n",
              "       -0.16772  ,  0.15089  , -0.34596  ,  0.21422  , -0.14757  ,\n",
              "        0.15441  ,  0.37975  , -0.59659  , -0.57633  ,  0.25796  ,\n",
              "       -0.55429  ,  0.12723  , -0.20154  , -0.068933 , -0.12297  ,\n",
              "        0.32023  , -0.30111  , -0.052125 , -0.37041  , -0.43252  ,\n",
              "        0.23192  ,  0.0036499, -0.66005  , -0.6475   ,  0.035395 ,\n",
              "       -0.0094254,  0.33469  , -0.19011  , -0.55927  , -0.75534  ,\n",
              "       -0.49898  ,  0.30307  ,  0.45647  , -0.3956   , -0.68686  ,\n",
              "       -0.20784  , -0.14301  ,  0.22278  ,  0.048205 ,  0.232    ,\n",
              "        0.013177 , -0.68994  , -0.61234  , -0.23618  ,  0.20272  ,\n",
              "        0.48777  , -0.2081   , -0.38003  , -0.26693  , -0.029294 ,\n",
              "       -0.75468  ,  0.22302  ,  0.17016  , -0.34758  ,  0.1835   ,\n",
              "       -0.4577   ,  0.45696  , -1.3146   , -0.93166  ,  0.49132  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Gw1HXnSRZ7RJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Ap-rQVDbAXWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "93896356-f899-4c8e-c82a-9c4281526c8a"
      },
      "cell_type": "code",
      "source": [
        "  modelGlove.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7698541283607483),\n",
              " ('monarch', 0.6843380928039551),\n",
              " ('throne', 0.6755735874176025),\n",
              " ('daughter', 0.6594556570053101),\n",
              " ('princess', 0.6520534753799438),\n",
              " ('prince', 0.6517034769058228),\n",
              " ('elizabeth', 0.6464517712593079),\n",
              " ('mother', 0.6311717629432678),\n",
              " ('emperor', 0.6106470823287964),\n",
              " ('wife', 0.6098655462265015)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "8DGo6E7YAXY_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "api.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "359Uesd-4qMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4b17786d-b965-4597-8753-7c73dc23be4e"
      },
      "cell_type": "code",
      "source": [
        "#preciso gerar o código que gerará o vetor Y. Primeiro vou trabalhar com apenas\n",
        "#o primeiro tag. Depois trabalharei com multi-label.\n",
        "df = pd.read_csv('ted_main.csv')\n",
        "#df.ratings[0]\n",
        "Y=[]\n",
        "tags = df['tags']\n",
        "for linha in tags:\n",
        "  linha=linha.lower()\n",
        "  linha=linha.replace(\"[\",\"\")\n",
        "  linha=linha.replace(\"]\",\"\")\n",
        "  linha=linha.replace(\"'\",\"\")\n",
        "  linhaSplitada=linha.split(\",\")\n",
        "  primeiroTagDaLinha=linhaSplitada[0]\n",
        "  Y.append(primeiroTagDaLinha)\n",
        "print(len(Y))\n",
        "print(Y)\n",
        "mySet=set(Y)\n",
        "print(len(mySet))#numero de tags diferentes\n",
        "print(mySet)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2550\n",
            "['children', 'alternative energy', 'computers', 'macarthur grant', 'africa', 'business', 'christianity', 'architecture', 'god', 'christianity', 'activism', 'ted prize', 'ted prize', 'demo', 'children', 'entertainment', 'creativity', 'macarthur grant', 'dna', 'business', 'business', 'collaboration', 'business', 'cognitive science', 'culture', 'climate change', 'astronomy', 'business', 'business', 'ted brain trust', 'business', 'cancer', 'aging', 'alternative energy', 'africa', 'business', 'business', 'nasa', 'climate change', 'aids', 'ted prize', 'ted prize', 'culture', 'culture', 'ted brain trust', 'choice', 'ted brain trust', 'business', 'culture', 'culture', 'business', 'aids', 'anthropology', 'activism', 'astronomy', 'collaboration', 'cities', 'business', 'dance', 'macarthur grant', 'macarthur grant', 'code', 'choice', 'africa', 'ted prize', 'activism', 'africa', 'ted brain trust', 'dna', 'brain', 'business', 'dna', 'ted brain trust', 'alternative energy', 'architecture', 'art', 'entertainment', 'business', 'brazil', 'biotech', 'dna', 'dna', 'art', 'animals', 'animals', 'biology', 'climate change', 'cello', 'ted brain trust', 'creativity', 'architecture', 'ted brain trust', 'entertainment', 'composing', 'composing', 'god', 'god', 'comedy', 'entertainment', 'business', 'entertainment', 'google', 'culture', 'alternative energy', 'collaboration', 'ted brain trust', 'ai', 'animals', 'collaboration', 'climate change', 'business', 'demo', 'buddhism', 'new york', 'culture', 'cello', 'design', 'africa', 'moon', 'ted brain trust', 'bioethics', 'design', 'aids', 'demo', 'entertainment', 'animation', 'culture', 'africa', 'africa', 'africa', 'africa', 'africa', 'africa', 'business', 'africa', 'africa', 'business', 'culture', 'books', 'africa', 'animals', 'ted brain trust', 'ted brain trust', 'entertainment', 'africa', 'art', 'asteroid', 'alternative energy', 'nasa', 'culture', 'aircraft', 'ai', 'art', 'cities', 'brain', 'business', 'brain', 'buddhism', 'business', 'dna', 'business', 'ted brain trust', 'climate change', 'design', 'africa', 'design', 'string theory', 'macarthur grant', 'education', 'brain', 'communication', 'children', 'south america', 'comedy', 'ants', 'entertainment', 'animals', 'art', 'architecture', 'culture', 'macarthur grant', 'culture', 'entertainment', 'art', 'business', 'business', 'brazil', 'macarthur grant', 'animals', 'business', 'live music', 'nasa', 'architecture', 'collaboration', 'collaboration', 'africa', 'ted brain trust', 'astronomy', 'children', 'alternative energy', 'demo', 'biology', 'architecture', 'ted prize', 'ted prize', 'africa', 'architecture', 'biology', 'culture', 'africa', 'art', 'education', 'animation', 'string theory', 'activism', 'business', 'creativity', 'google', 'africa', 'culture', 'string theory', 'string theory', 'entertainment', 'africa', 'biology', 'bacteria', 'macarthur grant', 'animals', 'google', 'environment', 'adventure', 'business', 'art', 'activism', 'africa', 'brain', 'ted brain trust', 'africa', 'anthropology', 'culture', 'computers', 'art', 'aging', 'animals', 'vaccines', 'business', 'conducting', 'activism', 'culture', 'entertainment', 'business', 'art', 'entertainment', 'africa', 'animals', 'culture', 'business', 'entertainment', 'planets', 'brain', 'christianity', 'comedy', 'brain', 'brain', 'africa', 'africa', 'africa', 'art', 'architecture', 'communication', 'brain', 'art', 'cello', 'astronomy', 'africa', 'animals', 'entertainment', 'animals', 'animals', 'entertainment', 'children', 'africa', 'animals', 'dna', 'business', 'business', 'education', 'africa', 'global issues', 'books', 'animals', 'children', 'animals', 'children', 'brain', 'activism', 'business', 'brain', 'children', 'communication', 'children', 'ted brain trust', 'ai', 'art', 'culture', 'architecture', 'ted prize', 'business', 'entertainment', 'cities', 'entertainment', 'physics', 'art', 'beauty', 'health', 'comedy', 'demo', 'culture', 'aids', 'culture', 'design', 'business', 'africa', 'charter for compassion', 'charter for compassion', 'charter for compassion', 'charter for compassion', 'charter for compassion', 'charter for compassion', 'business', 'culture', 'activism', 'art', 'senses', 'string theory', 'communication', 'mars', 'art', 'ted brain trust', 'business', 'africa', 'astronomy', 'business', 'entertainment', 'business', '\"alzheimers\"', 'complexity', 'cities', 'alternative energy', 'animation', 'culture', 'art', 'animals', 'books', 'ted brain trust', 'books', 'mars', 'biology', 'children', 'asia', 'dna', 'code', 'africa', 'design', 'design', 'beauty', 'art', 'art', 'business', 'art', 'design', 'culture', 'biology', 'philanthropy', 'business', 'dinosaurs', 'beauty', 'global issues', 'collaboration', 'energy', 'ted brain trust', 'creativity', 'art', 'art', 'business', 'biology', 'conducting', 'ted prize', 'ted prize', 'ted prize', 'aging', 'business', 'biodiversity', 'art', 'business', 'children', 'animals', 'activism', 'economics', 'entertainment', 'creativity', 'demo', 'activism', 'brain', 'business', 'animals', 'brain', 'animals', 'art', 'architecture', 'alternative energy', 'activism', 'culture', 'culture', 'aids', 'culture', 'business', 'adventure', 'creativity', 'design', 'architecture', 'global issues', 'macarthur grant', 'cognitive science', 'communication', 'alternative energy', 'biology', 'art', 'dance', 'adventure', 'art', 'business', 'africa', 'cities', 'new york', 'business', 'brain', 'culture', 'vaccines', 'astronomy', 'ted fellows', 'art', 'ted prize', 'natural resources', 'activism', 'books', 'art', 'aids', 'asia', 'new york', 'business', 'culture', 'books', 'planets', 'cars', 'children', 'united states', 'culture', 'biodiversity', 'guitar', 'activism', 'ted brain trust', 'art', 'books', 'community', 'architecture', 'architecture', 'beauty', 'animals', 'business', 'tedx', 'communication', 'god', 'design', 'tedx', 'brain', 'global issues', 'activism', 'global issues', 'economics', 'children', 'architecture', 'children', 'brain', 'asia', 'biology', 'anthropocene', 'art', 'biology', 'brain', 'anthropology', 'europe', 'culture', 'art', 'apes', 'art', 'business', 'business', 'biodiversity', 'entertainment', 'brain', 'engineering', 'africa', 'animation', 'anthropocene', 'global development', 'culture', 'design', 'entertainment', 'climate change', 'adventure', 'brain', 'asia', 'adventure', 'architecture', 'comedy', 'biology', 'creativity', 'internet', 'internet', 'africa', 'adventure', 'africa', 'asia', 'art', 'god', 'astronomy', 'ted brain trust', 'ancient world', 'tedx', 'africa', 'art', 'architecture', 'cities', 'body language', 'advertising', 'brain', 'birds', 'tedx', 'tedx', 'business', 'vaccines', 'economics', 'ancient world', 'architecture', 'depression', 'architecture', 'creativity', 'anthropology', 'business', 'chemistry', 'ted prize', 'culture', 'demo', 'biomimicry', 'environment', 'business', 'astronomy', 'asia', 'big problems', 'africa', 'culture', 'business', 'europe', 'astronomy', 'architecture', 'art', 'asia', 'comedy', 'art', 'design', 'egypt', 'corruption', 'asia', 'internet', 'biodiversity', 'art', 'asia', 'alternative energy', 'ted fellows', 'government', 'business', 'adventure', 'biology', 'art', 'tedx', 'animals', 'live music', 'culture', 'biology', 'business', 'activism', 'children', 'cities', 'books', 'astronomy', 'biology', 'asia', 'biology', 'live music', 'biology', 'education', 'cities', 'tedx', 'culture', 'ted fellows', 'business', 'books', 'big problems', 'collaboration', 'tedx', 'cancer', 'art', 'tedx', 'business', 'cities', 'business', 'activism', 'ted brain trust', 'tedx', 'business', 'tedmed', 'autism spectrum disorder', 'biology', 'art', 'brain', 'brain', 'business', 'data', 'astronomy', 'adventure', 'collaboration', 'business', 'internet', 'cities', 'agriculture', 'entertainment', 'internet', 'biology', 'business', 'biodiversity', 'computers', 'adventure', 'creativity', 'culture', 'brazil', 'business', 'mars', 'ted fellows', 'humanity', 'slavery', 'children', 'future', 'business', 'children', 'business', 'aids', 'design', 'ai', 'bees', 'literature', 'global issues', 'global issues', 'business', 'art', 'internet', 'animals', 'culture', 'ted fellows', 'business', 'business', 'culture', 'string theory', 'adventure', 'chemistry', 'comedy', 'big problems', 'tedx', 'fish', 'creativity', 'entertainment', 'evolution', 'ted brain trust', 'economics', 'tedx', 'children', 'humanity', 'cancer', 'culture', 'animals', 'alternative energy', 'dna', 'ted brain trust', 'tedx', 'ted fellows', 'aids', 'cities', 'internet', 'computers', 'fish', 'internet', 'curiosity', 'tedx', 'democracy', 'africa', 'creativity', 'anthropocene', 'architecture', 'god', 'art', 'animals', 'tedx', 'cancer', 'buddhism', 'art', 'cities', 'children', 'children', 'collaboration', 'tedx', 'fish', 'art', 'ted fellows', 'math', 'tedx', 'crime', 'culture', 'tedx', 'business', 'communication', 'culture', 'surveillance', 'islam', 'planets', 'brain', 'biology', 'business', 'tedx', 'ted brain trust', 'mission blue', 'brain', 'activism', 'business', 'slavery', 'god', 'ai', 'comedy', 'iran', 'tedx', 'complexity', 'climate change', 'brain', 'animals', 'tedx', 'business', 'big problems', 'anthropocene', 'brain', 'goal-setting', 'biodiversity', 'children', 'art', 'global issues', 'tedx', 'ted brain trust', 'mission blue', 'business', 'evolution', 'friendship', 'culture', 'books', 'collaboration', 'aids', 'aids', 'tedx', 'senses', 'biology', 'decision-making', 'brain', 'ptsd', 'tedx', 'anthropocene', 'economics', 'mission blue', 'africa', 'cancer', 'biology', 'tedx', 'ted fellows', 'art', 'internet', 'lgbt', 'business', 'biology', 'anthropocene', 'art', 'future', 'business', 'china', 'environment', 'tedx', 'animation', 'culture', 'culture', 'global issues', 'biology', 'entertainment', 'design', 'africa', 'tedx', 'big problems', 'ted fellows', 'code', 'senses', 'tedx', 'culture', 'global issues', 'tedx', 'drones', 'tedx', 'tedx', 'tedx', 'tedx', 'agriculture', 'africa', 'business', 'business', 'gender equality', 'crime', 'gender equality', 'tedx', 'children', 'tedx', 'ted brain trust', 'africa', 'gender equality', 'tedx', 'tedx', 'culture', 'business', 'tedx', 'tedx', 'business', 'tedx', 'activism', 'anthropology', 'anthropocene', 'communication', 'tedx', 'activism', 'health care', 'anthropocene', 'tedx', 'ai', 'asia', 'business', 'art', 'tedx', 'cancer', 'architecture', 'cities', 'advertising', 'tedx', 'culture', 'tedx', 'ai', 'community', 'architecture', 'tedx', 'charter for compassion', 'tedx', 'culture', 'business', 'foreign policy', 'business', 'surgery', 'anthropology', 'ted brain trust', 'entertainment', 'egypt', 'ted prize', 'egypt', 'aging', 'surgery', 'culture', 'culture', 'brain', 'tedx', 'brain', 'dark matter', 'community', 'tedx', 'entertainment', 'culture', 'ted fellows', 'bioethics', 'design', 'tedx', 'tedx', 'iran', 'animals', 'business', 'collaboration', 'education', 'culture', 'business', 'brand', 'creativity', 'activism', 'ted prize', 'internet', 'tedx', 'community', 'ted fellows', 'business', 'tedx', 'culture', 'design', 'exploration', 'business', 'bioethics', 'tedx', 'dna', 'demo', 'design', 'culture', 'culture', 'cities', 'ted fellows', 'string theory', 'beauty', 'animals', 'biology', 'body language', 'art', 'tedx', 'brain', 'architecture', 'senses', 'biology', 'culture', 'entertainment', 'art', 'vaccines', 'art', 'egypt', 'ted fellows', 'creativity', 'blindness', 'ted brain trust', 'philosophy', 'consumerism', 'tedx', 'dinosaurs', 'art', 'business', 'gender spectrum', 'communication', 'tedx', 'art', 'europe', 'ted fellows', 'design', 'design', 'cars', 'brain', 'culture', 'art', 'brain', 'ted en español', 'computers', 'tedx', 'activism', 'culture', 'ted brain trust', 'bees', 'consciousness', 'ted fellows', 'surveillance', 'culture', 'business', 'communication', 'internet', 'culture', 'complexity', 'animals', 'foreign policy', 'biology', 'happiness', 'culture', 'culture', 'consciousness', 'internet', 'biology', 'ted fellows', 'culture', 'human body', 'architecture', 'tedx', 'activism', 'ted fellows', 'art', 'cognitive science', 'dna', 'communication', 'ted fellows', 'buddhism', 'culture', 'tedx', 'biology', 'books', 'asia', 'internet', 'art', 'bioethics', 'culture', 'business', 'business', 'google', 'africa', 'animals', 'business', 'communication', 'creativity', 'africa', 'data', 'creativity', 'asia', 'tedx', 'consumerism', 'business', 'business', 'collaboration', 'brain', 'senses', 'business', 'culture', 'ted fellows', 'culture', 'energy', 'tedx', 'senses', 'ted fellows', 'culture', 'brain', 'drones', 'dna', 'art', 'surveillance', 'behavioral economics', 'cars', 'ai', 'entertainment', 'tedx', 'tedx', 'ai', 'ted fellows', 'archaeology', 'dna', 'tedx', 'internet', 'internet', 'human body', 'adventure', 'tedx', 'dna', 'comedy', 'tedx', 'tedx', 'nasa', 'creativity', 'creativity', 'tedx', 'tedx', 'crime', 'brain', 'tedx', 'senses', 'culture', 'africa', 'tedx', 'egypt', 'internet', 'macarthur grant', 'biotech', 'culture', 'tedx', 'tedx', 'ted fellows', 'nobel prize', 'business', 'agriculture', 'tedx', 'tedx', 'tedx', 'senses', 'senses', 'behavioral economics', 'global issues', 'tedx', 'comedy', 'tedx', 'europe', 'tedx', 'tedx', 'tedx', 'tedx', 'tedx', 'tedx', 'dna', 'tedx', 'tedx', 'tedx', 'ted fellows', 'atheism', 'internet', 'business', 'business', 'tedx', 'natural resources', 'tedyouth', '3d printing', 'culture', 'tedx', 'tedx', 'tedx', 'tedx', 'guns', 'cancer', 'tedx', 'surgery', 'dna', 'activism', 'tedx', 'tedx', 'algorithm', 'tedx', 'islam', 'tedx', 'creativity', 'tedx', 'tedx', 'activism', 'tedx', 'tedx', 'tedx', 'tedx', 'tedx', 'tedx', 'nasa', 'archaeology', 'tedx', 'art', 'behavioral economics', 'tedx', 'internet', 'tedx', 'tedyouth', 'biology', 'animals', 'cities', 'activism', 'ai', 'business', 'criminal justice', 'animation', 'big problems', 'activism', 'entertainment', 'tedx', 'tedx', 'ted fellows', 'planets', 'ted-ed', 'nobel prize', 'culture', 'business', 'brain', 'tedx', 'tedx', 'alternative energy', 'tedx', 'alternative energy', 'culture', 'tedx', 'tedx', 'alternative energy', 'internet', 'global issues', 'ted fellows', 'art', 'tedx', 'internet', 'ted-ed', 'ai', 'creativity', 'health care', 'human body', 'ted-ed', 'tedx', 'community', 'tedx', 'tedx', 'surgery', 'asia', 'health care', 'business', 'art', 'tedx', 'ted fellows', 'planets', 'tedx', 'ted-ed', 'brazil', 'activism', 'tedx', 'tedx', 'architecture', 'natural resources', 'art', 'internet', 'tedx', 'tedx', 'tedx', 'culture', 'data', 'adventure', 'brain', 'marketing', 'tedx', 'tedx', 'behavioral economics', 'ted fellows', 'creativity', 'ted fellows', 'ted prize', 'tedx', 'tedx', 'dna', 'tedx', 'entertainment', 'aids', 'comedy', 'tedx', 'ai', 'internet', 'egypt', 'tedx', 'culture', 'nasa', 'tedx', 'autism spectrum disorder', 'human body', 'culture', 'natural resources', 'architecture', 'autism spectrum disorder', 'design', 'global issues', 'art', 'africa', 'gender equality', 'tedx', 'tedx', 'tedx', 'bioethics', 'internet', 'internet', 'tedx', 'moon', 'tedx', 'ted prize', 'internet', 'code', 'business', 'brain', 'alternative energy', '\"alzheimers\"', 'tedx', 'ted fellows', 'ted fellows', 'aging', 'dna', 'tedx', 'internet', 'creativity', 'tedx', 'ted fellows', 'dna', 'global development', 'tedx', 'art', 'senses', 'tedx', 'tedx', 'collaboration', 'europe', 'art', 'innovation', 'algorithm', 'natural resources', 'tedx', 'asia', 'tedx', 'internet', 'asia', 'tedx', 'tedx', 'tedx', 'business', 'ted fellows', 'agriculture', 'art', 'slavery', 'tedx', 'europe', 'tedx', 'culture', 'tedx', 'art', 'business', 'ai', 'art', 'nasa', 'tedx', 'tedx', 'depression', 'asia', 'bioethics', 'art', 'data', 'environment', 'aging', 'senses', 'internet', 'africa', 'new york', 'business', 'ai', 'behavioral economics', 'internet', 'ai', 'activism', 'egypt', 'tedx', 'cities', 'body language', 'ted fellows', 'crime', 'code', 'art', 'business', 'internet', 'activism', 'collaboration', 'art', 'business', 'health', 'bees', 'corruption', 'internet', 'business', 'tedx', 'tedx', 'tedx', 'illusion', 'tedx', 'data', 'dna', 'tedx', 'autism spectrum disorder', 'tedx', 'ptsd', 'tedx', 'activism', 'tedx', 'art', 'global issues', 'communication', 'tedyouth', 'community', 'tedx', 'tedx', 'children', 'tedx', 'ted fellows', 'tedx', 'cars', 'human body', 'africa', 'culture', 'egypt', 'africa', 'business', 'ted fellows', 'tedyouth', 'aids', 'internet', 'middle east', 'tedx', 'culture', 'business', 'bioethics', 'children', 'economics', 'iran', 'creativity', 'dna', 'animation', 'global issues', 'tedx', 'tedx', 'aids', 'buddhism', 'tedx', 'bioethics', 'tedx', 'tedx', 'tedx', 'ptsd', 'tedx', 'surgery', 'culture', 'business', 'internet', 'gender equality', 'middle east', 'children', 'compassion', 'dna', 'art', '3d printing', 'europe', 'tedx', 'tedx', 'tedx', 'culture', 'tedx', 'bioethics', 'behavioral economics', 'tedx', 'data', 'tedx', 'children', 'tedx', 'ted prize', 'energy', 'ted brain trust', 'agriculture', 'oceans', 'agriculture', 'africa', 'bullying', 'business', 'tedx', 'ted brain trust', 'activism', 'ted fellows', 'internet', 'energy', 'asia', 'dna', 'collaboration', 'ted fellows', 'demo', 'agriculture', 'tedx', 'tedyouth', 'biology', 'technology', 'corruption', 'ted fellows', 'ai', 'community', 'robots', 'tedx', 'health care', 'activism', 'big problems', '\"alzheimers\"', 'anthropology', 'tedx', 'dance', 'culture', 'economics', 'business', 'internet', 'cars', 'culture', 'business', 'alternative energy', 'brazil', 'ted brain trust', 'children', 'tedx', 'china', 'ted brain trust', 'chemistry', 'children', 'education', 'business', 'business', 'culture', 'aging', 'culture', 'art', 'creativity', 'ted brain trust', 'global issues', 'art', 'aging', 'architecture', 'creativity', 'art', 'gender equality', 'tedx', 'creativity', 'children', 'behavioral economics', 'africa', 'animals', 'creativity', 'economics', 'drones', 'europe', 'drones', 'islam', 'business', 'africa', 'family', 'tedx', 'ted fellows', 'islam', 'health', 'art', 'bioethics', 'robots', 'asia', 'tedx', 'ted fellows', 'children', 'business', 'architecture', 'internet', 'internet', 'culture', 'animals', 'beauty', 'culture', 'bacteria', 'tedx', 'senses', 'design', 'children', 'advertising', 'culture', 'design', 'tedmed', 'creativity', 'tedx', 'creativity', 'tedx', 'senses', 'africa', 'health', 'autism spectrum disorder', 'tedx', 'tedx', 'biology', 'tedx', 'culture', 'humor', 'body language', 'business', 'art', 'activism', 'collaboration', 'blindness', 'big problems', 'crime', 'crime', 'agriculture', 'ted fellows', 'bioethics', 'cities', 'bioethics', 'curiosity', 'tedx', 'culture', 'tedx', 'ancient world', 'death', 'algorithm', 'art', 'nasa', 'debate', 'debate', 'architecture', 'foreign policy', 'disease', 'culture', 'cities', 'culture', 'cities', 'business', 'art', 'business', 'africa', 'books', 'string theory', 'africa', 'business', 'anthropocene', 'communication', 'adventure', 'asia', 'business', 'anthropology', 'animals', 'internet', 'entertainment', 'africa', 'blindness', 'africa', 'health', 'entertainment', 'animals', 'africa', 'tedx', 'drones', 'brain', 'aging', 'brazil', 'culture', 'business', 'tedx', 'entertainment', 'ted fellows', 'south america', 'africa', 'africa', 'tedyouth', 'tedx', 'consumerism', 'business', 'economics', 'tedx', 'design', 'tedx', 'aging', 'cities', 'comedy', 'tedx', 'business', 'health', 'tedx', 'ted fellows', 'business', 'bioethics', 'gender equality', 'tedx', 'ai', 'egypt', 'depression', 'business', 'communication', 'internet', 'crime', 'tedyouth', 'tedx', 'animation', 'gender', 'map', 'architecture', 'ai', 'ted fellows', 'tedx', 'design', 'business', 'biomimicry', 'tedx', 'biology', 'business', 'culture', 'gender equality', 'aging', 'cities', 'economics', 'tedyouth', 'business', 'surgery', 'business', 'internet', 'dna', 'ted fellows', 'disability', 'gender equality', 'africa', 'animals', 'brain', 'behavioral economics', 'internet', 'adventure', 'ted prize', 'surveillance', 'ai', 'education', 'ancient world', 'animals', 'security', 'dance', 'gender equality', 'dance', 'astronomy', 'activism', 'tedx', 'corruption', 'architecture', 'education', 'bacteria', 'senses', 'guns', 'introvert', 'culture', 'lgbt', 'planets', 'design', 'art', 'alternative energy', 'tedx', 'senses', 'creativity', 'autism spectrum disorder', 'dna', 'blindness', 'algorithm', 'future', 'business', 'entertainment', 'surveillance', 'google', 'live music', 'tedx', 'internet', 'culture', 'tedx', 'entertainment', 'business', 'tedx', 'gender equality', 'sports', 'ptsd', 'ptsd', 'environment', 'ted fellows', '\"alzheimers\"', 'creativity', 'ai', 'ted brain trust', 'culture', 'animals', 'gender equality', 'tedx', 'internet', 'ted fellows', 'creativity', 'comedy', 'atheism', 'tedx', 'choice', 'culture', 'animals', 'crime', 'tedx', 'astronomy', 'tedx', 'culture', 'tedx', 'biology', 'foreign policy', 'behavioral economics', 'tedx', 'business', 'computers', 'choice', 'history', 'entertainment', 'brain', 'surgery', 'ted fellows', '3d printing', 'comedy', 'business', 'business', 'europe', 'ted fellows', 'tedx', 'human body', 'business', 'tedx', 'democracy', 'poetry', 'internet', 'ted fellows', 'books', 'ted fellows', 'ted fellows', 'future', 'africa', 'tedx', 'communication', 'comedy', 'philosophy', 'aging', 'ted fellows', 'tedx', 'middle east', 'new york', 'global issues', 'ted fellows', 'health', 'astronomy', 'books', 'manufacturing', 'tedx', 'anthropocene', 'data', 'middle east', 'death', 'architecture', 'ted fellows', 'tedx', 'global issues', 'brain', 'live music', 'economics', 'beauty', 'government', 'business', 'internet', 'brain', 'children', 'biotech', 'syria', 'ted fellows', 'data', 'africa', 'ted fellows', 'adventure', 'humor', 'crime', 'ted fellows', 'animals', 'cancer', 'activism', 'cities', 'biodiversity', 'business', 'architecture', 'art', 'economics', 'economics', 'crime', 'business', 'creativity', 'business', 'astronomy', 'tedx', 'ted fellows', 'autism spectrum disorder', 'medicine', 'tedx', 'ted books', 'activism', 'adventure', 'business', 'animals', 'dance', 'advertising', 'ted fellows', 'tedx', 'surveillance', 'cities', 'tedx', 'ai', 'tedx', 'activism', 'brazil', 'tedyouth', 'tedx', 'ted fellows', 'cities', 'middle east', 'tedyouth', 'climate change', 'business', 'climate change', 'cities', 'ted en español', 'communication', 'tedyouth', 'global development', 'gender equality', 'brain', 'government', 'africa', 'ted fellows', 'activism', 'tedx', 'tedyouth', 'business', 'tedyouth', 'tedyouth', 'business', 'tedyouth', 'ted books', 'tedx', 'brain', 'islam', 'nasa', 'tedx', 'ted books', 'middle east', 'ted en español', 'new york', 'entertainment', 'ted fellows', 'climate change', 'humor', 'internet', 'brazil', 'africa', 'ted books', 'tedx', 'tedx', 'business', 'tedx', 'tedx', 'blindness', 'demo', 'communication', 'ai', 'crime', 'ted prize', 'architecture', 'entrepreneur', 'ted fellows', 'blindness', 'asia', 'ted fellows', 'ted brain trust', 'ted fellows', 'ted en español', 'behavioral economics', 'nasa', 'global issues', 'activism', 'tedx', 'investment', 'mars', 'inequality', 'nasa', 'entertainment', 'inequality', 'ted fellows', 'ai', 'ted fellows', 'animals', 'criminal justice', 'agriculture', 'computers', 'ted fellows', 'ted fellows', 'adventure', 'bees', 'architecture', 'cities', 'live music', 'gender spectrum', 'ted fellows', 'animals', 'culture', 'creativity', 'crime', 'tedx', 'planets', 'culture', 'business', 'behavioral economics', 'design', 'surveillance', 'education', 'politics', 'culture', 'animals', 'gender spectrum', 'compassion', 'ted books', 'autism spectrum disorder', 'ted fellows', 'entertainment', 'gender equality', 'ted books', 'disease', 'cars', 'economics', 'global issues', 'empathy', 'peace', 'math', 'children', 'addiction', 'tedx', 'slavery', 'culture', 'culture', 'internet', 'animals', 'culture', 'gender', 'tedx', 'ted fellows', 'evolution', 'middle east', 'united states', 'ted fellows', 'aids', 'autism spectrum disorder', 'art', 'bioethics', 'surveillance', 'art', 'biology', 'vaccines', 'disaster relief', 'leadership', 'planets', 'gender equality', 'internet', 'human origins', 'google', 'ted books', 'death', 'gender equality', 'foreign policy', 'activism', 'gender equality', 'tedx', 'tedx', 'middle east', 'death', 'climate change', 'aging', '\"alzheimers\"', 'culture', 'tedx', 'tedx', 'tedx', 'big problems', 'ted books', 'architecture', 'aging', 'guitar', 'big problems', 'tedx', 'tedx', 'europe', 'ted fellows', 'criminal justice', 'bioethics', 'tedx', 'south america', 'big problems', 'ted fellows', 'ted fellows', 'europe', 'brain', 'activism', 'middle east', 'brazil', 'internet', 'art', 'gender equality', 'google', 'god', 'ptsd', 'brand', 'future', 'big problems', 'animals', 'europe', 'books', 'business', 'art', 'ted fellows', 'tedx', 'ai', 'tedx', 'ted fellows', 'tedyouth', 'tedyouth', 'tedyouth', 'animals', 'mars', 'ted fellows', 'gender equality', 'europe', 'internet', 'art', 'tedx', 'big bang', 'tedx', 'planets', 'tedx', 'comedy', 'cognitive science', 'tedx', 'egypt', 'architecture', 'tedyouth', 'africa', 'ai', 'africa', 'christianity', 'europe', 'adventure', 'debate', 'animals', 'activism', 'internet', 'bioethics', 'addiction', 'africa', 'tedx', 'united states', 'tedx', 'tedx', 'big problems', 'slavery', 'surgery', 'tedx', 'children', 'astronomy', 'beauty', 'alternative energy', 'islam', 'business', 'aids', 'gender spectrum', 'adventure', 'tedyouth', 'internet', 'europe', 'brand', 'children', 'vaccines', 'tedx', 'tedx', 'buddhism', 'god', 'behavioral economics', 'internet', 'ted fellows', 'debate', 'senses', 'criminal justice', 'tedx', 'ancient world', 'africa', 'nasa', 'brazil', 'debate', 'planets', 'business', 'business', 'internet', 'internet', 'art', 'nasa', 'activism', 'internet', 'activism', 'mars', 'big problems', 'big problems', 'communication', 'alternative energy', 'brain', 'aging', 'dna', 'activism', 'ted books', 'ted fellows', 'internet', 'choice', 'big problems', 'dna', 'tedmed', 'communication', 'activism', 'adventure', 'tedx', 'bioethics', 'brain', 'africa', 'big problems', 'body language', 'ted fellows', 'ptsd', 'tedx', 'internet', 'ted books', 'brain', 'adventure', 'activism', 'collaboration', 'africa', 'africa', 'activism', 'africa', 'collaboration', 'google', 'china', 'tedx', 'ted fellows', 'africa', 'big problems', 'ted fellows', 'art', 'internet', 'tedx', 'ted fellows', 'children', 'ted books', 'architecture', 'blindness', 'ai', 'tedx', 'blindness', 'criminal justice', 'middle east', 'europe', 'internet', 'senses', 'middle east', 'aging', 'anthropocene', 'natural resources', 'adventure', 'agriculture', 'egypt', 'internet', 'biology', 'beauty', 'ai', 'biology', 'natural resources', 'africa', 'mars', 'natural resources', 'mars', 'south america', 'collaboration', 'blindness', 'business', 'tedx', 'ted fellows', 'internet', 'human body', 'global issues', 'tedx', 'ted books', 'big problems', 'business', 'children', 'united states', 'tedx', 'south america', 'internet', 'children', 'natural resources', 'africa', 'brain', 'tednyc', 'tedx', 'new york', 'dna', 'internet', 'tednyc', 'africa', 'dna', 'ai', 'internet', 'crispr', 'tednyc', 'tedx', 'blindness', 'adventure', 'communication', 'internet', 'adventure', 'ptsd', 'business', 'tedx', 'behavioral economics', 'tedmed', 'ai', 'islam', 'tednyc', 'biology', 'africa', 'tedx', 'tedx', 'art', 'tedx', 'internet', 'slavery', 'big problems', 'beauty', 'ted fellows', 'gender equality', 'islam', 'tednyc', 'ted residency', 'ptsd', 'activism', 'internet', 'adventure', 'children', 'tedx', 'gender equality', 'dna', 'tedx', 'alternative energy', 'africa', 'tedx', 'tednyc', 'business', 'gender equality', 'tedx', 'tedx', 'aging', 'bioethics', 'tedx', 'ai', 'ptsd', 'children', 'choice', 'tedx', 'gender equality', 'mars', 'communication', 'nobel prize', 'anthropology', 'collaboration', 'children', 'tedx', 'tedx', 'chemistry', 'tedx', 'tedx', 'africa', 'gender equality', 'gender equality', 'anthropocene', 'debate', 'aids', 'macarthur grant', 'computers', 'europe', 'bacteria', 'egypt', 'tedx', 'tedx', 'guns', 'mars', 'ai', 'activism', 'biology', 'tedx', 'middle east', 'activism', 'guitar', 'media', 'internet', 'criminal justice', 'ai', 'big problems', 'criminal justice', 'tedx', 'africa', 'activism', 'tedx', 'ted residency', 'tednyc', 'children', 'guitar', 'curiosity', 'tednyc', 'gender equality', 'gender equality', 'ai', 'tedx', 'tedmed', 'nasa', 'gender equality', 'ted en español', 'art', 'tedx', 'surgery', 'children', 'tedx', 'foreign policy', 'tednyc', 'ted-ed', 'gender equality', 'addiction', 'dna', 'live music', 'africa', 'gender equality', 'united states', 'tedx', 'internet', 'tedmed', 'tednyc', 'tedx', 'tedx', 'tednyc', 'ted residency', 'africa', 'tednyc', 'astronomy', 'tednyc', 'addiction', 'criminal justice', 'africa', 'tedx', 'christianity', 'gender equality', '\"alzheimers\"', 'mars', 'tedx', 'internet', 'africa', 'ted fellows', 'tedmed', 'biology', 'south america', 'anthropocene', 'aging', 'ai', 'tedmed', 'big problems', 'tedx', 'activism', 'capitalism', 'africa', 'united states', 'tedx', 'collaboration', 'tedx', 'ai', 'criminal justice', 'africa', 'history', 'tednyc', 'internet', 'tednyc', 'autism spectrum disorder', 'ai', 'christianity', 'choice', 'adventure', 'tedx', 'bioethics', 'tednyc', 'africa', 'ai', 'ted books', 'tednyc', 'tedx', 'children', 'ted fellows', 'community', 'ted en español', 'tedx', 'addiction', 'anthropocene', 'brain', 'tedx', 'united states', 'blindness', 'architecture', 'dna', 'algorithm', 'human body', 'ted fellows', 'demo', 'art', 'south america', 'tednyc', 'ai', 'ai', 'tedmed', 'aging', 'moon', 'art', 'bacteria', 'ted fellows', 'brain', 'criminal justice', 'art', 'architecture', 'tedx', 'aging', 'ted-ed', 'algorithm', 'ai', 'art', 'ai', 'biology', 'africa', 'tedx', 'africa', 'tedx', 'ai', 'tedx', 'tedx', 'tedx', 'architecture', 'tedx', 'algorithm', 'ted en español', 'tedx', 'africa', 'tedx', 'happiness', 'tedx', 'africa', 'business', 'tedx', 'africa', 'dna', 'africa', 'ted residency', 'mars', 'ai', 'internet', 'cities']\n",
            "194\n",
            "{'complexity', 'activism', 'christianity', 'evolution', 'aids', 'biotech', 'physics', 'robots', 'human origins', 'nasa', 'archaeology', 'biology', 'tednyc', 'guns', 'empathy', 'behavioral economics', 'compassion', 'history', 'gender spectrum', 'decision-making', 'architecture', 'surgery', 'innovation', 'cars', 'friendship', 'consumerism', 'consciousness', 'dna', 'agriculture', 'bees', 'cello', 'ted en español', 'literature', 'senses', 'anthropocene', 'alternative energy', 'astronomy', 'disability', 'string theory', 'environment', 'tedmed', 'dance', 'iran', 'media', 'bacteria', 'biomimicry', 'aircraft', 'marketing', 'climate change', 'code', 'slavery', 'medicine', 'middle east', 'gender', 'crime', 'adventure', 'algorithm', 'foreign policy', 'birds', 'advertising', 'exploration', 'drones', 'crispr', 'cognitive science', 'culture', 'nobel prize', 'illusion', 'health', 'egypt', 'community', '3d printing', 'moon', 'government', 'google', 'biodiversity', 'humor', 'tedyouth', 'democracy', 'conducting', 'cancer', 'brain', 'manufacturing', 'brand', 'death', 'asteroid', 'united states', 'aging', 'depression', 'family', 'ted residency', 'comedy', 'animation', 'atheism', 'entrepreneur', 'ted fellows', 'ted prize', 'debate', 'big bang', 'criminal justice', 'goal-setting', 'math', 'poetry', 'south america', 'blindness', 'design', 'philosophy', 'future', 'macarthur grant', 'corruption', 'syria', 'bioethics', 'fish', 'choice', 'ted-ed', 'leadership', 'live music', 'ted brain trust', 'ants', 'dark matter', 'demo', 'natural resources', 'humanity', 'vaccines', 'curiosity', 'ancient world', 'mars', 'mission blue', 'security', 'brazil', 'cities', 'map', 'engineering', 'anthropology', 'ai', 'health care', 'asia', 'collaboration', 'sports', 'disaster relief', 'buddhism', 'oceans', 'education', 'investment', 'internet', 'disease', 'global development', 'surveillance', 'children', 'body language', 'lgbt', 'technology', 'capitalism', 'gender equality', 'autism spectrum disorder', 'ted books', 'big problems', 'computers', 'china', 'introvert', 'islam', 'communication', 'books', 'animals', 'charter for compassion', 'dinosaurs', 'creativity', 'business', 'africa', 'energy', 'entertainment', 'new york', 'art', 'inequality', 'human body', 'global issues', 'chemistry', 'tedx', '\"alzheimers\"', 'apes', 'economics', 'data', 'ptsd', 'composing', 'peace', 'philanthropy', 'politics', 'god', 'addiction', 'europe', 'happiness', 'beauty', 'guitar', 'planets', 'bullying'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "axkGaSABF3Pz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "QTD_TEXTOS=len(Y)\n",
        "QTD_PALAVRAS=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-IGB8DBAAXby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#construção da tabela entrada da regressão X\n",
        "Y\n",
        "X=np.array(ndmin=2)\n",
        "numeroDaLinha=-1\n",
        "for text in texts:\n",
        "  numeroDaLinha+=1\n",
        "  for i in range(QTD_PALAVRAS):#vou pegar as 30 primeiras palavras de cada texto\n",
        "    palavra = text[i]\n",
        "    vetorDaPalavra = modelo[palavra]\n",
        "    X[numeroDaLinha][i]=vetorDaPalavra\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVDR_9EGAXin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28t1DaieuFjc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmVgOmfQfr3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6uE_tHKMfr6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://radimrehurek.com/gensim/models/keyedvectors.html é melhor usar full model \n",
        "# quando You need the full model to train or update vectors.\n",
        "#The reason for separating the trained vectors into KeyedVectors is that if you don’t need \n",
        "#the full model state any more (don’t need to continue training), the state can discarded, \n",
        "#resulting in a much smaller and faster object that can be mmapped for lightning fast loading \n",
        "#and sharing the vectors in RAM between processes:\n",
        "\n",
        "\n",
        "\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "#depreciado\n",
        "#model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  \n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import KeyedVectors\n",
        "#Vectors exported by the Facebook and Google tools do not support further training, \n",
        "#but you can still load them into KeyedVectors.\n",
        "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8161anmnD3X8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7LmXgpqdFf0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model = gensim.models.Word2Vec(processed_corpus, min_count=1)\n",
        "model = gensim.models.Word2Vec(['carro','barco'], min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GwTDoYnFNn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=gensim.models.Word2Vec.load_word2vec_format()#depreciado"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqqSZm0yD36k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0aWenmihD39e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2wHw84h9fMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "api.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eocOwaFxfr_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6g4vCeOzfsBy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z--Liov4DI4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load an example dataset\n",
        "from vega_datasets import data\n",
        "cars = data.cars()\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "import altair as alt\n",
        "alt.Chart(cars).mark_bar().encode(\n",
        "  x=alt.X('Miles_per_Gallon', bin=True),\n",
        "  y='count()',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YSgNhnqwunDx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://github.com/RaRe-Technologies/gensim\n",
        "#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/gensim%20Quick%20Start.ipynb\n",
        "\n",
        "\n",
        "\n",
        "raw_corpus = [\"Human machine interface for lab abc computer applications\",\n",
        "             \"A survey of user opinion of computer system response time\",\n",
        "             \"The EPS user interface management system\",\n",
        "             \"System and human system engineering testing of EPS\",              \n",
        "             \"Relation of user perceived response time to error measurement\",\n",
        "             \"The generation of random binary unordered trees\",\n",
        "             \"The intersection graph of paths in trees\",\n",
        "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "             \"Graph minors A survey\"]\n",
        "\n",
        "# Create a set of frequent words\n",
        "stoplist = set('for a of the and to in'.split(' '))\n",
        "# Lowercase each document, split it by white space and filter out stopwords\n",
        "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
        "         for document in raw_corpus]\n",
        "\n",
        "# Count word frequencies\n",
        "from collections import defaultdict\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "# Only keep words that appear more than once\n",
        "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
        "processed_corpus\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qKHTczmo0vX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#These datasets contain information about all audio-video recordings of TED Talks uploaded to the official TED.com website until September 21st, 2017. The TED main dataset contains information about all talks including number of views, number of comments, descriptions, speakers and titles. The TED transcripts dataset contains the transcripts for all talks available on TED.com.\n",
        "#There are two CSV files:\n",
        "#ted_main.csv - Contains data on actual TED Talk metadata and TED Talk speakers.\n",
        "#transcripts.csv - Contains transcript and URL information for TED Talks\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDqgXpl6uJje",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New \n"
      ]
    }
  ]
}